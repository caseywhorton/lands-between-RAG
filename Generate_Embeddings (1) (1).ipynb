{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2d2102-4d73-4cf5-aeb4-7d7f167037b8",
   "metadata": {},
   "source": [
    "# Text Embeddings in Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46176edd-b64b-4b00-841c-53e532454326",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (2.4.1.post100)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1\n",
      "Collecting pinecone\n",
      "  Downloading pinecone-6.0.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.11/site-packages (from pinecone) (2025.1.31)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.11/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from pinecone) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Downloading pinecone-6.0.1-py3-none-any.whl (421 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone\n",
      "Successfully installed pinecone-6.0.1 pinecone-plugin-interface-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d874fc5-a251-453e-8fa5-f708e0874feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import boto3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    Metric,\n",
    "    DeletionProtection,\n",
    "    VectorType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf7f510-e47d-4324-bc8e-ed58cfbedf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_4bEMf4_9Sn1sJL6a6vJL1Tu6nRqmssTwc5guzsTsyXKYJ7U8Vf14Hh7SdiX1oAgBg3Kuwn\"\n",
    "os.environ[\"PINECONE_ENV\"] = \"us-east1-aws\"\n",
    "s3_client = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408de0b-c157-4b4d-989d-2c5327fe9c03",
   "metadata": {},
   "source": [
    "## Pinecone for Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1119d0a1-f2da-46a7-b066-b172da4ddd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone and AWS\n",
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "              environment=\"us-east1-aws\")\n",
    "\n",
    "# Define the Pinecone index name and embeddings dimension\n",
    "index_name = 'lands-between-eldenringbuilds'\n",
    "embedding_dimension = 768  # Update based on the embedding model you're using (e.g., 768 for BERT)\n",
    "\n",
    "# Create the Pinecone index if it doesn't exist\n",
    "if not any(index['name'] == index_name for index in pc.list_indexes()):\n",
    "    pc.create_index(index_name, \n",
    "                    dimension=embedding_dimension,\n",
    "                    spec=ServerlessSpec(cloud=CloudProvider.AWS,\n",
    "                                        region=AwsRegion.US_EAST_1)\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f3ad365-1929-4823-9ba4-b6d437e3fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Create an index instance\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('bert-base-uncased')  # You can use any SentenceTransformer model\n",
    "\n",
    "def retrieve_s3_files(bucket_name, prefix):\n",
    "    \"\"\" Retrieve list of text file keys from an S3 bucket \"\"\"\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith('.json')]\n",
    "\n",
    "\n",
    "def read_s3_file(bucket_name, file_key):\n",
    "    \"\"\" Read a file from S3 and return its content \"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    return response['Body'].read().decode('utf-8')\n",
    "\n",
    "\n",
    "def generate_embeddings(text_data):\n",
    "    \"\"\" Generate embeddings for a given text \"\"\"\n",
    "    return model.encode(text_data).tolist()\n",
    "\n",
    "def insert_into_pinecone(vectors):\n",
    "    \"\"\"Upsert embeddings into Pinecone.\"\"\"\n",
    "    index.upsert(vectors=vectors)\n",
    "    print(f\"Inserted {len(vectors)} records into Pinecone.\")\n",
    "\n",
    "\n",
    "def process_s3_files(bucket_name, prefix):\n",
    "    \"\"\"Process S3 files containing Reddit JSON data, generate embeddings, and store in Pinecone \"\"\"\n",
    "\n",
    "    file_keys = retrieve_s3_files(bucket_name, prefix)  # Get list of files from S3\n",
    "    vectors = []  # Store vectors to insert into Pinecone\n",
    "\n",
    "    for file_key in file_keys:\n",
    "        text_data = read_s3_file(bucket_name, file_key)  # Read JSON file from S3\n",
    "        reddit_posts = json.loads(text_data)  # Parse JSON\n",
    "\n",
    "        for post in reddit_posts:\n",
    "            post_id = post[\"id\"]\n",
    "            subreddit = post[\"metadata\"][\"subreddit\"]\n",
    "            url = post[\"metadata\"][\"url\"]\n",
    "            author = post[\"metadata\"][\"author\"]\n",
    "            timestamp = post[\"metadata\"][\"timestamp\"]\n",
    "\n",
    "            # Process title embedding\n",
    "            title_embedding = generate_embeddings(post[\"title\"])\n",
    "            vectors.append({\n",
    "                \"id\": f\"{post_id}-title\",\n",
    "                \"values\": title_embedding,\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"title\",\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"url\": url,\n",
    "                    \"author\": author,\n",
    "                    \"timestamp\": timestamp\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Process body embedding\n",
    "            if post[\"body\"]:\n",
    "                body_embedding = generate_embeddings(post[\"body\"])\n",
    "                vectors.append({\n",
    "                    \"id\": f\"{post_id}-body\",\n",
    "                    \"values\": body_embedding,\n",
    "                    \"metadata\": {\n",
    "                        \"type\": \"body\",\n",
    "                        \"subreddit\": subreddit,\n",
    "                        \"url\": url,\n",
    "                        \"author\": author,\n",
    "                        \"timestamp\": timestamp\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            # Process comments embeddings\n",
    "            for idx, comment in enumerate(post[\"comments\"]):\n",
    "                comment_embedding = generate_embeddings(comment)\n",
    "                vectors.append({\n",
    "                    \"id\": f\"{post_id}-comment-{idx}\",\n",
    "                    \"values\": comment_embedding,\n",
    "                    \"metadata\": {\n",
    "                        \"type\": \"comment\",\n",
    "                        \"subreddit\": subreddit,\n",
    "                        \"url\": url,\n",
    "                        \"author\": author,\n",
    "                        \"timestamp\": timestamp\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        if vectors:  # Only insert if we have embeddings to upsert\n",
    "            insert_into_pinecone(vectors)\n",
    "            print(f\"Processed {len(file_keys)} files and inserted {len(vectors)} embeddings into Pinecone.\")\n",
    "        else:\n",
    "            print(\"No valid embeddings found to insert into Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23d0b0c6-0668-47ba-8331-2e37e98166d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 53 records into Pinecone.\n",
      "Processed 2 files and inserted 53 embeddings into Pinecone.\n",
      "Inserted 106 records into Pinecone.\n",
      "Processed 2 files and inserted 106 embeddings into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Example usage s3://webscrape-lands-between/reddit_data/EldenringBuilds/\n",
    "S3_BUCKET_NAME = 'webscrape-lands-between'\n",
    "PREFIX = \"reddit_data/EldenringBuilds\"\n",
    "process_s3_files(S3_BUCKET_NAME, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6b14e-0b8e-4013-a594-c720e383f41a",
   "metadata": {},
   "source": [
    "## Pinecone for Webpage Scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "890139ab-d590-4c05-9eb4-bf8024a2a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert into pineceone.\n",
      "Processed 391 files and inserted embeddings into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone and AWS\n",
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "              environment=\"us-east1-aws\")\n",
    "\n",
    "# Define the Pinecone index name and embeddings dimension\n",
    "index_name = 'lands-between-index'\n",
    "embedding_dimension = 768  # Update based on the embedding model you're using (e.g., 768 for BERT)\n",
    "\n",
    "# Create the Pinecone index if it doesn't exist\n",
    "if not any(index['name'] == index_name for index in pc.list_indexes()):\n",
    "    pc.create_index(index_name, \n",
    "                    dimension=embedding_dimension,\n",
    "                    spec=ServerlessSpec(cloud=CloudProvider.AWS,\n",
    "                                        region=AwsRegion.US_EAST_1)\n",
    "                   )\n",
    "\n",
    "# Create an index instance\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('bert-base-uncased')  # You can use any SentenceTransformer model\n",
    "\n",
    "\n",
    "def retrieve_s3_files(bucket_name, prefix):\n",
    "    \"\"\" Retrieve list of text file keys from an S3 bucket \"\"\"\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith('.txt')]\n",
    "\n",
    "\n",
    "def read_s3_file(bucket_name, file_key):\n",
    "    \"\"\" Read a file from S3 and return its content \"\"\"\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    return response['Body'].read().decode('utf-8')\n",
    "\n",
    "\n",
    "def generate_embeddings(text_data):\n",
    "    \"\"\" Generate embeddings for a given text \"\"\"\n",
    "    return model.encode(text_data).tolist()\n",
    "\n",
    "\n",
    "def insert_into_pinecone(embeddings, metadatas):\n",
    "    # Use string for id (convert integers to strings)\n",
    "    vectors = [\n",
    "        {\"id\": str(i), \"values\": embedding, \"metadata\": metadata} \n",
    "        for i, (embedding, metadata) in enumerate(zip(embeddings, metadatas))\n",
    "    ]\n",
    "\n",
    "    # Upsert the vectors\n",
    "    index.upsert(vectors=vectors)\n",
    "\n",
    "\n",
    "def process_s3_files(bucket_name, prefix):\n",
    "    \"\"\" Process S3 files, generate embeddings, and store them in Pinecone \"\"\"\n",
    "\n",
    "    file_keys = retrieve_s3_files(bucket_name, prefix)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "    \n",
    "    for file_key in file_keys:\n",
    "        #print(f\"Generating Embeddings for {file_key}\")\n",
    "        text_data = read_s3_file(bucket_name, file_key)\n",
    "        embeddings = generate_embeddings(text_data)\n",
    "\n",
    "        # Create metadata for each embedding (you can include other info here if needed)\n",
    "        metadata = {\"file_name\": file_key}\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_metadatas.append(metadata)\n",
    "    print(\"Upsert into pineceone.\")\n",
    "    # Insert the embeddings into Pinecone\n",
    "    insert_into_pinecone(all_embeddings, all_metadatas)\n",
    "\n",
    "    print(f\"Processed {len(all_embeddings)} files and inserted embeddings into Pinecone.\")\n",
    "\n",
    "# Example usage\n",
    "S3_BUCKET_NAME = 'webscrape-lands-between'\n",
    "PREFIX = \"scraped_data\"\n",
    "process_s3_files(S3_BUCKET_NAME, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67721db-36b6-4a78-8e3c-1865ac62103a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
